--- 
title: "Методы снижения размерности в данных в макроэкономике" 
author: "Михаил Гареев" 
date: '28 марта 2019 г ' 
output: html_document 
---


## Актуальность

Одна из важных проблем, которые возникают при оценке
макроэкономических моделей, --- это малое количество наблюдений (например, стран
в мире меньше трёхсот, при этом полноценные и долгосрочные статистические данные
собраны далеко не для всех государств) и относительно большое количество
возможных параметров. Использование стандартных средств эконометрического
анализа (например, линейной регрессии) не всегда оправдано в такой ситуации. Для
получения корректных оценок необходимо каким-то образом уменьшать количество
параметров модели, т.е. сокращать размерность данных, сохраняя при этом
имеющуюся информацию. Эта работа рассматривает применение различных методов
снижения размерности в макроэкономике.

## Цель работы

* Проверка некоторых гипотез макроэкономики при помощи методов
снижения размерности в данных и создание на их основе предсказательных моделей
для макроэкономических показателей.

## Задачи работы 

* Обзор методов снижения размерности (LASSO, Post-LASSO,
Dantzig Selector и др.).

* Применение этих методов для оценки макроэкономических
зависимостей, анализ результатов, сравнение с другими методами оценивания и с
результатами, полученными ранее. 
* Построение предсказательных моделей.
* Создание процедуры мэтчинга стран на основе их макроэкономических показателей.

## Методы снижения размерности

Прежде, чем перейти к обзору методов снижения
размерности, дадим формальное определение **разреженной линейной
эконометрической модели с высокой размерностью в данных**. Так называют линейную
модель регрессии с большим количеством объясняющих переменных $p$, которое,
возможно, больше, чем размер выборки $n$, но только небольшое число $s<n$ этих
регрессоров важно для объяснения основных свойств модели. Последнее делает
возможным эффективно оценивать такую модель, находя аппроксимацию множества $s$
ненулевых коэффициентов с помощью методов снижения размерности, которые
рассматриваются в этой работе. Представим общий вид такой модели: 


\[ y_i = {x_i}^{'}
\beta_0 + \varepsilon_i, \epsilon_i \sim N(0, \sigma^2), \beta_0 \in
\mathbb{R}^p, i = 1, \dots, n, \] 

где $y_i$ --- это значения объясняемой
переменной, $x_i$ --- это значения $p$-размерной объясняющей переменной,
$\varepsilon_i$ --- значения независимых случайных ошибок в каждом наблюдении
$i$, при этом возможно, что $p \geq n$, но только $s<n$ компонентов вектора
$\beta_0$ не равны $0$.

При рассмотрении таких моделей естественным образом возникает вопрос: каким
образом можно идентфицировать ненулевые коэффициенты? Ответ помогают найти методы
снижения размерности в данных.

Рассмотрим следующую оптимизационную задачу (**Oracle Problem**):
\[ \min_{\beta \in
\mathbb{R}^p} \mathbb{E}_n\left[ (y_i - {x_i}^{'} \beta)^2 \right] + \sigma^2
\frac{\left\lVert \beta \right\rVert_0}{n}, 
\]
где $\left\lVert \beta \right\rVert_0$ --- это количество ненулевых компонентов в векторе $\beta$, некоторое обобщение понятия нормы для степени $0$. Напомним, что семейство Гёльдероых норм для вектора $x$ определется как $\left\lVert x \right\rVert_p = \sqrt[p]{\sum_i|x_i|^p}$,, где обычно $p \geq 1$.
Решение данной выше задачи оптимизации --- это результат баланса между ошибкой регрессии и количеством ненулевых коэффициентов из вектора $\beta$. Поэтому фактически применение некоторого метода снижения размерности для оценки линейной регресии сводится к нахождению оптимума для эмпирического аналога Oracle Problem.

Рассмотрим более подробно эти методы.

* Классическая оценка на основе AIC/ BIC, являющаяся решением задачи:
\[ \min_{\beta \in
\mathbb{R}^p} \sum_i=1^n \left[ (y_i - {x_i}^{'} \beta)^2 \right] +  \frac{\lambda}{n} \left\lVert \beta \right\rVert_0, 
\]
где $\lambda$ --- некоторый параметр, задающий штраф за дополнительного регрессоров. Такая оценка имеет хорошее теоретическое обоснование, но не всегда может быть вычислена при больших $n$ или $p$.

* Оценка на основе LASSO (Least Absolute shrinkage and selection operator) выглядит следующим образом:
\[
\overhat{\beta} \in \arg \min_{\beta \in
\mathbb{R}^p} \sum_i=1^n \left[ (y_i - {x_i}^{'} \beta)^2 \right] +  \frac{\lambda}{n} \left\lVert \beta \right\rVert_1,
\]
Такой метод минимизирует выпуклую функцию, и, таким образом, с точки зрения скорости вычисления и вообще наличия самой возможности решения задачи за разумное время, этот способ лучше, чем использование AIC или BIC. Параметр $\lambda$ при этом выбирается на основе некоторых алгоритмов и зависит от уровня шума в данных.

## Ridge
Эта оценка похожа по формуле на LASSO, но, вместе с тем, совсем другая. В такой регрессии никакие коэффиценты не зануляются. Можно сказать, что это менее консервативная модель.

## Elnet
Объединяет в себе две модели ridge и lasso. параметры находятся через кросс-валидацию. В наших данных часто была близкой к LASSO.

* Оценка с применением Post-LASSO. Идея метода Post-LASSO залючается в том, чтобы сначала исключить с помощью LASSO нулевых регрессоров, а после этого использовать МНК, заранее полагая исключенные параметры равными нулю. Можно показать, что при относительно низком шуме в данных и при правильной идентификации ненулевых параметров Post-LASSO помогает существенно скорректировать LASSO-оценку, но, при этом, высокий шум в данных и неправильная идентификация коэффицентов модели могут привести к смещению оценок даже по сравнению с LASSO-регрессией.

## Random forest
<<Высаживание>> случайного леса состоит из двух этапов:

* Данные случаным образом разбиваются на N подвыборок (с повторением), на каждой строится решающее дерево (при этом при построении каждой вершины дерева выбирается переменная из $p_0$ переменных, случайным образом выбираемых из $p$ регрессоров (как правило, в задачах регрессии $p_0 = p/3$). Переменная и её разделяющее значение выбирается таким образом, чтобы уменьшение энтропии относительно объясняемой переменной в двух новых подвыборках было максимальным из возможных (например, можно измерять разброс при помощи разных метрик, например, при помощи суммы квадратов ошибок RSS). Дерево строится, например, пока каждому разбиению не будет соответствовать не более, чем $k$ наблюдений.
* В качестве предсказанного значения $\hat{y_i}$ выбираются усреднённые значения показаний по всем деревьям.

## BA
Байесовский подход в эконометрике предполагает наличие априорного распределения для каждого параметра модели, которое в соответствии с имеющимися данными корректируется и в результате получается апостерироное распределение.
Такой подход, например, получать оценки параметров, даже если переменных больше, чем наблюдений, или позволяет напрямую ставить вопрос о вероятности того, что какой-то коэффициент в модели равен нулю.

Регрессия пик-плато предполагает, что априорное распределение каждого параметра задаётся кусочной функцией (либо коэффициент точно равен нулю, либо его значение почти неизвестно):
    
    
По методу Монте Карло по схеме марковской цепи много (тысячи раз) генерируются выборки из апостериорных (т.е. условных при имеющейся информации) распределений параметров модели. Таким образом можно получать вероятности того, что какие-либо коэффициенты равны 0, или же получать, используя полученные на основании выборок матожидания коэффициентов в качестве оценок коэффициентов, прогнозные значения объясняемой переменной.


## unemp
## pca
Можно предположить, что среди данных, используемых в исследовании, много сильно кореллированных переменных. В этом случае логично преобразовать данные, аппроксимировав их, снижая размерность и минимально теряя информацию. Самый популярный способ сделать это --- метод главных компонент (principal component analysis, PCA). Можно представить его как последовательную минимизацию суммы квадратов отклонений старых значений от новых или замена матрицы $X_{n \times k}$ на матрицу $\hat{{n \times k}}$ ранга $p < k$, так, чтобы:

\begin{equation}
    \min \sum_{j = 1}^k \sum_{i = 1}^{n}(x_{ij} - \hat{x_{ij}})^2
\end{equation}

## pc for data


## rmse


## результаты
* Методы снижения размерности (LASSO, Post-LASSO, Ridge, Elastic Net, Random Forest) потенциально представляют собой мощный инструмент для нахождения и проверки макроэкономических зависимостей.
 * Из использованных методов лучшие результаты при прогнозировании инфляции в России показывают модели LASSO и Random Forest. На разных горизонтах планирования  (кроме диапазона с 9 до 15 месяцев) хотя бы одна из них показывала лучшие результаты, чем модель-бенчмарк (ARMA).
## Источники

## Бонус








